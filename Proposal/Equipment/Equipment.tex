
At UTA we have five separate labs (electronics, optics, etc.) with several high speed oscilloscopes including one with 6 GHz bandwidth, power supplies, logic modules, and electronic racks. In addition we host the ATLAS tier-2 computing center with large amount of computing resources.

Equipment currently available in PI Asaadi's liquid argon lab include
\begin{itemize}
\item Cryofab 3048 540 Liter Cryostat with top plate consisting of 1 CF 14'' flange, 3 FC 4.5'' flanges, and 3 FC 2.75'' flanges including internal fill line and bottom drain

\item Cryofab 1636 117 Liter Cryostat with top plate consisting of 2 CF 4.5'' flanges and 2 CF 2.75'' flanges including internal fill line and bottom drain

\item Custom 70 Liter Cylindrical Cryo-Vessel with 14'' custom top flange with 1 CF 4.5'' flange, 3 FC 2.75'' flanges and 2 CF 1.33'' flanges

\item Custom 40 Liter Cylindrical Cryo Vessel with Two 750 cm$^3$ liquid argon Molecular Sieve filters with Sigma Aldrich 4A bead 8-12 mesh filter material
Two 750 cm$^3$ liquid argon activated copper filters with Research Catalyst Q-5 copper catalyst 14x28 mesh bead filter material

\item Mc Master Easy-Stor In Lab Hydraulic Crane (2000 lb maximum weight) 10’ 8” max height

\item CryoMech PT60 cold head with air cooled helium compressor

\item HiCube80 Eco Dry Turbo Pump 
 
\item Glassman PS/FR50R06 50kV power supply

\item Stanford Research Systems Residual Gas Analyzer A200

\item Teledyne Lecroy HDO 4104A 1 GHz 2.5 GS/s Oscilloscope 

\item CAEN A1702 32 Channel SiPM Readout front end board

\item CAEN DT1470 4 channel 8kV HV power supply

\item MicroBooNE CF 14” cold electronics readout flange with intermediate amplifiers and 16 LARASIC4 motherboards

\item Relevant cryogenic hosing and valves associated with the UTA liquid argon cryogenic facility

\end{itemize}


A small High Performance cluster dedicated to                                                                                                              
GPU computing, primarily for Deep Learning studies, which are used by
over two dozen HEP collaborators over six experiments. The cluster
provides 54 processing cores (108 job slots due to hyper-threading) 
over 4 systems, with more than 4 GB of RAM per core, 2 GB/s SSD caches
in front of 50 TB of storage, 10 Gb interconnectivity, 4 NVidia (3 
Kepler, 1 Maxwell) GPUs, 3 AMD GPUs, and an Intel Phi (Knights
Corner).





